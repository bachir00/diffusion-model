# üé® Mod√®le de Diffusion - Documentation du Code Source

Ce dossier contient l'impl√©mentation compl√®te d'un **mod√®le de diffusion (DDPM - Denoising Diffusion Probabilistic Model)** pour la g√©n√©ration d'images, optimis√© pour l'entra√Ænement sur GPU.

---

## üìÅ Structure des Fichiers

### Fichiers Principaux

| Fichier | Description |
|---------|-------------|
| **`config.py`** | Configuration centralis√©e du mod√®le et de l'entra√Ænement |
| **`model.py`** | Architecture U-Net avec attention et blocs r√©siduels |
| **`diffusion.py`** | Impl√©mentation du processus de diffusion DDPM |
| **`train.py`** | Script d'entra√Ænement principal avec optimisations |
| **`dataset.py`** | Chargement et pr√©traitement des donn√©es |
| **`generate.py`** | G√©n√©ration d'images depuis un checkpoint entra√Æn√© |
| **`sample.py`** | √âchantillonnage rapide d'images depuis un checkpoint |
| **`schedules.py`** | Diff√©rents schedules de bruit (linear, cosine, quadratic) |
| **`util.py`** | Utilitaires (EMA, sauvegarde d'images) |

### Dossiers

- **`checkpoints128_pro/`** : Checkpoints du mod√®le entra√Æn√© (128√ó128)
- **`samples128_pro/`** : √âchantillons g√©n√©r√©s pendant l'entra√Ænement

---

## üß† Architecture du Mod√®le

### 1. **U-Net (`model.py`)**

Le c≈ìur du syst√®me est un **U-Net** (architecture encoder-decoder avec skip connections) :

#### Composants Principaux

```
Input (3√óH√óW) 
    ‚Üì
[Timestep Embedding] ‚Üí Encodage sinuso√Ødal du temps
    ‚Üì
[Encoder] 
    ‚Ä¢ ResidualBlocks (avec time conditioning)
    ‚Ä¢ AttentionBlocks (self-attention)
    ‚Ä¢ Downsampling (r√©duction r√©solution)
    ‚Üì
[Middle] 
    ‚Ä¢ ResidualBlock + Attention + ResidualBlock
    ‚Üì
[Decoder]
    ‚Ä¢ Upsampling (augmentation r√©solution)
    ‚Ä¢ ResidualBlocks + Skip Connections
    ‚Ä¢ AttentionBlocks
    ‚Üì
Output (3√óH√óW) ‚Üí Pr√©diction du bruit
```

#### Classes Importantes

- **`SiLU`** : Activation Swish (x * sigmoid(x))
- **`timestep_embedding()`** : Encodage positionnel sinuso√Ødal du timestep
- **`ResidualBlock`** : 
  - Bloc r√©siduel avec normalisation de groupe
  - Injection du timestep via projection lin√©aire
  - Support du scale-shift normalization (optionnel)
- **`AttentionBlock`** : Self-attention spatial (single-head ou multi-head)
- **`UNet`** : Architecture compl√®te avec skip connections

**Param√®tres cl√©s** :
- `model_channels` : Nombre de canaux de base (ex: 192)
- `channel_mult` : Multiplicateurs de canaux par niveau (ex: [1,2,3,4])
- `num_res_blocks` : Nombre de blocs r√©siduels par niveau
- `attention_resolutions` : R√©solutions o√π appliquer l'attention (ex: [16])

---

### 2. **Processus de Diffusion (`diffusion.py`)**

Impl√©mentation du **DDPM (Denoising Diffusion Probabilistic Model)** :

#### Forward Process (ajout de bruit)
```python
x_t = ‚àö(Œ±ÃÖ_t) * x_0 + ‚àö(1 - Œ±ÃÖ_t) * Œµ
```
o√π :
- `x_0` : image originale
- `x_t` : image bruit√©e au timestep t
- `Œµ` : bruit gaussien
- `Œ±ÃÖ_t` : produit cumul√© des alphas

#### Reverse Process (d√©bruitage)
Le mod√®le apprend √† pr√©dire le bruit `Œµ` pour reconstituer l'image progressivement.

**M√©thodes principales** :
- `q_sample()` : Ajoute du bruit √† une image (forward)
- `p_mean_variance()` : Calcule la distribution pour le d√©bruitage (reverse)
- `forward()` : Calcule la loss d'entra√Ænement (MSE sur le bruit pr√©dit)
- `sample()` : G√©n√®re de nouvelles images depuis du bruit pur

---

### 3. **Schedules de Bruit (`schedules.py`)**

Trois types de schedules pour contr√¥ler l'ajout progressif de bruit :

| Schedule | Description | Usage |
|----------|-------------|-------|
| **Linear** | Augmentation lin√©aire de Œ≤_start √† Œ≤_end | Simple, stable |
| **Cosine** | Suit une courbe cosinus (plus lent au d√©but) | **Recommand√©** - meilleure qualit√© |
| **Quadratic** | Augmentation quadratique | Compromis |

```python
betas = get_beta_schedule("cosine", timesteps=1000)
```

---

## üîß Configuration (`config.py`)

Le fichier `config.py` contient plusieurs classes de configuration adapt√©es √† diff√©rentes contraintes :

### Exemple : `DiffusionConfig` (128√ó128 optimis√©)

```python
class DiffusionConfig:
    # DONN√âES
    data_dir = "../data/train/cats_cleaned/good"
    image_size = 128
    in_channels = 3
    out_channels = 3
    
    # MOD√àLE (U-Net)
    model_channels = 192           # Canaux de base
    channel_mult = [1, 2, 3, 4]    # 4 niveaux de r√©solution
    num_res_blocks = 2             # Blocs par niveau
    attention_resolutions = [16]   # Attention √† 16√ó16
    dropout = 0.1
    
    # DIFFUSION
    timesteps = 1000               # Nombre d'√©tapes de diffusion
    beta_schedule = "cosine"       # Type de schedule
    
    # ENTRA√éNEMENT
    batch_size = 8
    num_epochs = 500
    learning_rate = 2e-4
    num_workers = 4
    
    # OPTIMISATIONS
    use_fp16 = False               # Mixed precision (√©conomie VRAM)
    gradient_accumulation_steps = 1
    gradient_clip = 1.0
    ema_decay = 0.999             # Exponential Moving Average
```

**Notes importantes** :
- Les configurations comment√©es montrent l'√©volution des param√®tres test√©s
- Ajustez `model_channels` et `attention_resolutions` selon votre VRAM
- Plus `timesteps` est √©lev√©, meilleure est la qualit√© (mais plus lent)

---

## üöÄ Entra√Ænement (`train.py`)

### Fonctionnalit√©s

#### 1. **Optimisations GPU**
- **Mixed Precision (FP16)** : R√©duit la VRAM de ~40%
- **Gradient Accumulation** : Simule de plus gros batch_size
- **cuDNN Benchmark** : Optimisation automatique des kernels
- **TF32** : Activ√© automatiquement sur GPU Ampere (RTX 30/40)
- **Pin Memory** : Transfert CPU‚ÜíGPU plus rapide

#### 2. **EMA (Exponential Moving Average)**
Maintient une version liss√©e des poids du mod√®le pour de meilleures g√©n√©rations :
```python
ema = EMA(unet, decay=0.999)
ema.update()  # Pendant l'entra√Ænement
ema.apply_shadow()  # Pour la g√©n√©ration
```

#### 3. **Checkpoint & Sampling**
- Sauvegarde automatique tous les N epochs
- G√©n√©ration d'√©chantillons pour suivre la progression
- Sauvegarde du meilleur mod√®le selon la loss

#### 4. **Learning Rate Warmup**
Augmentation progressive du learning rate au d√©but pour stabiliser l'entra√Ænement.

### Boucle d'Entra√Ænement

```python
for epoch in range(num_epochs):
    for batch in dataloader:
        # Forward pass avec mixed precision
        with autocast(device_type='cuda', enabled=use_fp16):
            loss = ddpm(batch)
        
        # Backward pass
        scaler.scale(loss).backward()
        
        # Gradient clipping + optimization step
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(unet.parameters(), gradient_clip)
        scaler.step(optimizer)
        scaler.update()
        
        # Update EMA
        ema.update()
```

### Reprise d'Entra√Ænement

Le script supporte la reprise depuis un checkpoint :
```python
train(resume_from_checkpoint="checkpoints/checkpoint_epoch_0100.pt")
```

---

## üìä Dataset (`dataset.py`)

### Classe `CatDataset`

Charge et pr√©traite les images :

```python
dataset = CatDataset(
    data_dir="../data/train/cats",
    image_size=128,
    augment=True  # Active l'augmentation de donn√©es
)
```

**Transformations appliqu√©es** :
1. Resize vers la taille cible
2. Center Crop
3. Random Horizontal Flip (si `augment=True`)
4. Conversion en Tensor
5. Normalisation dans [-1, 1] : `(x - 0.5) / 0.5`

**Fonction `denormalize()`** : Reconvertit de [-1,1] vers [0,1] pour l'affichage.

---

## üé® G√©n√©ration (`generate.py` & `sample.py`)

### G√©n√©ration d'Images

```python
# Avec generate.py (complet)
generate_images(
    checkpoint_path="checkpoints/best_model.pt",
    num_images=16,
    output_dir="generated",
    device="cuda"
)

# Avec sample.py (rapide)
sample_from_checkpoint(
    ckpt_path="checkpoints/final_unet.pt",
    out_path="samples/output.png",
    num_samples=16
)
```

**Diff√©rences** :
- `generate.py` : Plus complet, g√®re plusieurs formats de sortie
- `sample.py` : Plus simple, pour tests rapides

### Processus de G√©n√©ration

1. Commence avec du bruit pur : `x_T ~ N(0, I)`
2. Pour chaque timestep t de T √† 0 :
   - Pr√©dit le bruit avec le U-Net
   - Calcule `x_{t-1}` en retirant le bruit pr√©dit
3. Retourne `x_0` (image finale)

---

## üõ†Ô∏è Utilitaires (`util.py`)

### Classe `EMA`

Maintient une moyenne mobile exponentielle des poids :
```python
ema = EMA(model, decay=0.9999)
ema.update()          # Met √† jour la moyenne
ema.apply_shadow()    # Applique les poids EMA
ema.restore()         # Restaure les poids originaux
```

**Avantage** : G√©n√®re des images plus stables et de meilleure qualit√©.

### Fonction `save_image_grid()`

Sauvegarde une grille d'images :
```python
save_image_grid(
    images,                 # Tensor [N, C, H, W]
    path="output.png",
    nrow=4                  # Images par ligne
)
```

---

## üí° Utilisation Pratique

### 1. Entra√Æner un Mod√®le

```bash
# Depuis le dossier src/
python train.py
```

### 2. Reprendre un Entra√Ænement

Modifier `train.py` :
```python
if __name__ == '__main__':
    train(resume_from_checkpoint="checkpoints/checkpoint_epoch_0100.pt")
```

### 3. G√©n√©rer des Images

```bash
python generate.py
```

### 4. Ajuster la Configuration

√âditer `config.py` et changer la classe active :
```python
# Utiliser DiffusionConfig au lieu de DiffusionConfig1
from config import DiffusionConfig
```

---

## üìà M√©triques & Suivi

### Logs d'Entra√Ænement

Le fichier `training.log` contient :
- Loss par epoch/step
- Vitesse d'entra√Ænement (images/sec)
- Utilisation m√©moire
- Temps par epoch

### Visualisation

Les √©chantillons dans `samples128_pro/` permettent de :
- Suivre la progression visuelle
- D√©tecter l'overfitting
- Comparer diff√©rentes configurations

---

## ‚öôÔ∏è Optimisations & Astuces

### Pour R√©duire la VRAM

1. **Diminuer `batch_size`** : 8 ‚Üí 4 ou 2
2. **Activer FP16** : `use_fp16 = True`
3. **R√©duire `model_channels`** : 192 ‚Üí 128 ou 96
4. **Moins d'attention** : `attention_resolutions = [16]` au lieu de `[16, 8]`
5. **Moins de niveaux** : `channel_mult = [1,2,3]` au lieu de `[1,2,3,4]`

### Pour Am√©liorer la Qualit√©

1. **Plus de timesteps** : 1000 ‚Üí 1500 ou 2000
2. **Schedule cosine** : Meilleur que linear
3. **EMA decay √©lev√©** : 0.9999 au lieu de 0.999
4. **Plus de donn√©es** : Dataset plus large et vari√©
5. **Plus d'epochs** : Entra√Æner plus longtemps

### Pour Acc√©l√©rer l'Entra√Ænement

1. **cuDNN benchmark** : `cudnn_benchmark = True`
2. **Persistent workers** : Dans DataLoader
3. **Pin memory** : `pin_memory = True`
4. **Gradient accumulation** : Si batch_size limit√©

---

## üîç Points Techniques Avanc√©s

### Skip Connections dans U-Net

Les skip connections relient l'encoder au decoder :
```python
# Encoder: sauvegarde les features
hs.append(h)

# Decoder: r√©cup√®re et concat√®ne
skip = hs.pop()
h = torch.cat([h, skip], dim=1)
```

**Pourquoi ?** Pr√©serve les d√©tails haute fr√©quence perdus lors du downsampling.

### Time Conditioning

Le timestep est inject√© dans chaque ResidualBlock :
```python
temb = timestep_embedding(t, dim)     # Encodage sinuso√Ødal
temb = time_mlp(temb)                 # Projection MLP
h = h + temb_proj(temb)[:,:,None,None]  # Ajout spatial
```

**Pourquoi ?** Le mod√®le doit savoir √† quel niveau de bruit il travaille.

### Attention Mechanism

Self-attention pour capturer les relations spatiales :
```python
Q = q(x), K = k(x), V = v(x)
Attention = softmax(Q¬∑K^T / ‚àöd) ¬∑ V
```

**Co√ªt** : Quadratique en r√©solution ‚Üí appliqu√© seulement √† basses r√©solutions (ex: 16√ó16).

---

## üêõ Troubleshooting

| Probl√®me | Solution |
|----------|----------|
| **Out of Memory (CUDA OOM)** | R√©duire batch_size, activer FP16, diminuer model_channels |
| **Loss ne descend pas** | V√©rifier learning rate, augmenter warmup_steps, v√©rifier donn√©es |
| **Images floues** | Augmenter timesteps, utiliser schedule cosine, entra√Æner plus longtemps |
| **Mode collapse** | Augmenter dropout, v√©rifier diversit√© du dataset, r√©duire learning rate |
| **Artefacts en damier** | Remplacer ConvTranspose2d par Upsample + Conv2d |

---

## üìö R√©f√©rences Th√©oriques

- **Paper DDPM** : [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (Ho et al., 2020)
- **Improved DDPM** : [Improved Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2102.09672) (Nichol & Dhariwal, 2021)
- **Architecture U-Net** : [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)

---

## üéØ R√©sum√© du Pipeline

```
1. Configuration (config.py)
   ‚Üì
2. Chargement donn√©es (dataset.py)
   ‚Üì
3. Cr√©ation mod√®les (model.py + diffusion.py)
   ‚Üì
4. Boucle d'entra√Ænement (train.py)
   ‚Ä¢ Forward: ajout de bruit + pr√©diction
   ‚Ä¢ Backward: MSE loss + optimisation
   ‚Ä¢ EMA update + sampling p√©riodique
   ‚Üì
5. Sauvegarde checkpoints
   ‚Üì
6. G√©n√©ration finale (generate.py)
```

---

## üìù Notes Finales

Ce code est **production-ready** avec :
- ‚úÖ Support GPU optimis√© (FP16, gradient accumulation)
- ‚úÖ Reprise d'entra√Ænement robuste
- ‚úÖ Logging d√©taill√©
- ‚úÖ EMA pour stabilit√©
- ‚úÖ Configurations multiples

**Configuration recommand√©e pour RTX 4070 8GB** :
- `image_size = 128`
- `model_channels = 192`
- `batch_size = 8`
- `attention_resolutions = [16]`
- `use_fp16 = True` (si besoin)

---

